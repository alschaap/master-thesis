{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def DSSPseq(dssp,chain=\"A\"):\n",
    "    \"\"\"This function returns the sequence of a given chain in a dssp object\"\"\"\n",
    "\n",
    "    return ''.join([dssp.property_dict[x][1] for x in dssp.keys() if x[0]==chain ])\n",
    "\n",
    "def fastaSeq(fasta, chain=\"A\"):\n",
    "\n",
    "    from Bio import SeqIO\n",
    "    import re\n",
    "    for record in SeqIO.parse(fasta, \"fasta\"):\n",
    "        outer = re.compile(\"[Cc]hains? ([A-Z,]+)\")\n",
    "        m = outer.search(record.description.split(\"|\")[1])\n",
    "        if re.search(chain,m.groups()[0]):\n",
    "            return record.seq\n",
    "    return ''\n",
    "\n",
    "def oneHot(residue):\n",
    "    mapping = dict(zip(\"ACDEFGHIKLMNPQRSTVWY\", range(20)))\n",
    "    if residue in \"ACDEFGHIKLMNPQRSTVWY\":\n",
    "        return np.eye(20)[mapping[residue]]\n",
    "    else:\n",
    "        return np.zeros(20)\n",
    "    \n",
    "    \n",
    "def reverseOneHot(encoding):\n",
    "    mapping = dict(zip(range(20),\"ACDEFGHIKLMNPQRSTVWY\"))\n",
    "    seq=''\n",
    "    for i in range(len(encoding)):\n",
    "        if np.max(encoding[i])>0:\n",
    "            seq+=mapping[np.argmax(encoding[i])]\n",
    "    return seq\n",
    "\n",
    "def dssp8(classLetter):    \n",
    "    mapping = dict(zip(\"GHIBESTC\", range(8))) \n",
    "    if classLetter in \"GHIBESTC\":\n",
    "        return np.eye(8)[mapping[classLetter]]\n",
    "    else:\n",
    "        return np.zeros(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 10257.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 10272.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 10287.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 10302.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 10413.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 10581.\n",
      "  PDBConstructionWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LE 20\n",
      "LE0 (20,)\n",
      "M [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f61870d147f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m        \u001b[0malignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverseOneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdsspList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#this is a norml element, add dssp to it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#this is just to show how to download files and parse them with Bio.PDB\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "from Bio import pairwise2\n",
    "import wget\n",
    "urlBase='https://files.rcsb.org/download/'\n",
    "fastaBase='https://www.rcsb.org/fasta/entry/'\n",
    "#dataFolder=\"/Users/pamar/Desktop/projects/august/data/\"\n",
    "currentStructure='1OAT'\n",
    "\n",
    "filename=wget.download(urlBase+currentStructure+'.pdb', out=\"/Users/pamar/Downloads/\"+currentStructure+\".pdb\")\n",
    "sequenceFilename=wget.download(fastaBase+currentStructure, out=\"/Users/pamar/Downloads/\"+currentStructure+\".fasta\")\n",
    "\n",
    "p = PDBParser()\n",
    "structure = p.get_structure(currentStructure, filename)\n",
    "model = structure[0]\n",
    "dssp = DSSP(model, filename)\n",
    "seq1=DSSPseq(dssp,\"A\")\n",
    "seq2=fastaSeq(sequenceFilename,\"A\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alignments = pairwise2.align.globalms(seq1, seq2,1,-1,-5,-.5,penalize_extend_when_opening=True,penalize_end_gaps=False)\n",
    "dsspCont=0\n",
    "dsspList=[]\n",
    "for i in range(len(alignments[0][0])):\n",
    "#    print(alignments[0][0][i])\n",
    "    if (alignments[0][0][i]==\"-\"):\n",
    "        #this is a disordered residue, add a dssp element like this\n",
    "        print(        alignments[0][1][i],oneHot(alignments[0][1][i]), reverseOneHot(oneHot(alignments[0][1][i])))\n",
    "\n",
    "        dsspList.append([oneHot(alignments[0][1][i])])\n",
    "    else:\n",
    "        #this is a norml element, add dssp to it\n",
    "        print(        alignments[0][1][i],oneHot(alignments[0][1][i]))\n",
    "        print(dssp.property_dict[dssp.keys()[dsspCont]])\n",
    "        dsspCont+=1\n",
    "\n",
    "reverseOneHot(dsspList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dsspList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " 'P',\n",
       " '-',\n",
       " 0.2867647058823529,\n",
       " -47.0,\n",
       " 118.6,\n",
       " 0,\n",
       " 0.0,\n",
       " 490,\n",
       " -0.1,\n",
       " 0,\n",
       " 0.0,\n",
       " 489,\n",
       " -0.1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dssp.property_dict[dssp.keys()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "length = []\n",
    "\n",
    "\n",
    "pdbList=[\"a\",\"b\",\"cc\"]\n",
    "\n",
    "\n",
    "def processPdb(pdb,chain,fasta):\n",
    "    #function to process a PDB file, returns the formatted dssp\n",
    "    \n",
    "    return [pdb,chain,fasta]\n",
    "\n",
    "\n",
    "def appList(l):\n",
    "    length.append(l)\n",
    "\n",
    "def startProcessing():\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        pdbFiles = [f for f in pdbList ]\n",
    "#        breakpoint()\n",
    "        future_proc = {executor.submit(processPdb, f,\"A\",\"X\"): f for f in pdbFiles}\n",
    "        for future in concurrent.futures.as_completed(future_proc):\n",
    "            appList(future.result())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    startProcessing()\n",
    "    print(len(length))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: starting\n",
      "ThreadPoolExecutor-0_0: sleeping 5ThreadPoolExecutor-0_1: sleeping 4\n",
      "main: unprocessed results <generator object Executor.map.<locals>.result_iterator at 0x7fa3beed3fc0>\n",
      "main: waiting for real results\n",
      "\n",
      "ThreadPoolExecutor-0_1: done with 4\n",
      "ThreadPoolExecutor-0_1: sleeping 3\n",
      "ThreadPoolExecutor-0_0: done with 5\n",
      "ThreadPoolExecutor-0_0: sleeping 2\n",
      "ThreadPoolExecutor-0_1: done with 3\n",
      "ThreadPoolExecutor-0_1: sleeping 1\n",
      "ThreadPoolExecutor-0_0: done with 2\n",
      "ThreadPoolExecutor-0_1: done with 1\n",
      "main: results: [0.5, 0.4, 0.3, 0.2, 0.1]\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def task(n):\n",
    "    print('{}: sleeping {}'.format(\n",
    "        threading.current_thread().name,\n",
    "        n)\n",
    "    )\n",
    "    time.sleep(n / 10)\n",
    "    print('{}: done with {}'.format(\n",
    "        threading.current_thread().name,\n",
    "        n)\n",
    "    )\n",
    "    return n / 10\n",
    "\n",
    "\n",
    "ex = futures.ThreadPoolExecutor(max_workers=2)\n",
    "#ex = futures.ProcessPoolExecutor(max_workers=2)\n",
    "\n",
    "\n",
    "print('main: starting')\n",
    "results = ex.map(task, range(5, 0, -1))\n",
    "print('main: unprocessed results {}'.format(results))\n",
    "print('main: waiting for real results')\n",
    "real_results = list(results)\n",
    "print('main: results: {}'.format(real_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pamar/Desktop/projects/august/script\n",
      "Downloading PDB structure '12AS'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 5676.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 5708.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 5740.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 5842.\n",
      "  PDBConstructionWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDB structure '16PK'...\n",
      "Finished with 50 redundancy data set\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBList\n",
    "import os\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "import pickle\n",
    "from Bio.PDB import PDBParser\n",
    "#from Bio.PDB.DSSP import DSSP\n",
    "from Bio import pairwise2\n",
    "\n",
    "\n",
    "\n",
    "SeqID50 = []\n",
    "Sequence50 = []\n",
    "for seq1 in SeqIO.parse(\"../data/test.fasta\", \"fasta\"):\n",
    "    Sequence50.append(str(seq1.seq))\n",
    "    SeqID50.append(seq1.id)\n",
    "SeqDict50 = dict(zip(SeqID50,Sequence50))\n",
    "p = PDBParser()\n",
    "pdbl=PDBList()\n",
    "OHCAA = []\n",
    "OHCSS = []\n",
    "for pdb in SeqID50:\n",
    "    currentStructure = pdb[:4]\n",
    "    pdbl.retrieve_pdb_file(currentStructure,pdir='PDB',file_format='pdb')\n",
    "    os.chdir(owd + '/PDB')\n",
    "    structure = p.get_structure(currentStructure,'pdb' + currentStructure + '.ent')\n",
    "    model = structure[0]\n",
    "    dssp = DSSP(model, 'pdb' + currentStructure + '.ent')\n",
    "    seq1=DSSPseq(dssp,pdb[-1])\n",
    "    seq2=SeqDict50[pdb]\n",
    "    alignments = pairwise2.align.globalms(seq1, seq2,1,-1,-5,-.5,penalize_extend_when_opening=True,penalize_end_gaps=False)\n",
    "    dsspCont=0\n",
    "    dsspList=[]\n",
    "    a = np.array([])\n",
    "    b = np.array([])\n",
    "    os.remove(owd +'/PDB/'+'pdb'+currentStructure+'.ent')\n",
    "    os.chdir(owd)\n",
    "    dsspCont = 0\n",
    "    for i in range(len(alignments[0][0])):\n",
    "        if alignments[0][0][i] == '-':\n",
    "            #error, this should be the real residue not -\n",
    "            a = np.hstack((a,oneHot(alignments[0][1][i])))\n",
    "\n",
    "            b = np.hstack((b,dssp8('-')))\n",
    "        else:\n",
    "            #this is a norml element, add dssp to it\n",
    "            a = np.hstack((a,oneHot(alignments[0][1][i])))\n",
    "            #OHCSSList.append([dssp.property_dict[dssp.keys()[dsspCont]][2]])\n",
    "            c = dssp.property_dict[dssp.keys()[dsspCont]][2]\n",
    "            b = np.hstack((b,dssp8(c)))\n",
    "            #print(c)\n",
    "            dsspCont+=1\n",
    "    OHCAA.append(a.reshape(len(alignments[0][0]),20))\n",
    "    OHCSS.append(b.reshape(len(alignments[0][0]),8))\n",
    "maxlength = max([x.shape[0] for x in OHCAA])\n",
    "# Extrapolating sequences\n",
    "for i in range(len(OHCAA)):\n",
    "    if maxlength > OHCAA[i].shape[0]:\n",
    "        diff = np.zeros((maxlength-OHCAA[i].shape[0],20))\n",
    "        OHCAA[i] = np.vstack((OHCAA[i],diff))\n",
    "        diff = np.zeros((maxlength-OHCSS[i].shape[0],8))\n",
    "        OHCSS[i] = np.vstack((OHCSS[i],diff))\n",
    "# os.chdir(owd)\n",
    "# pickle.dump([OHCAA,OHCSS],open('red50.pickle','wb'))\n",
    "#print(OHCAA)\n",
    "#print(OHCSS)\n",
    "print('Finished with 50 redundancy data set')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHCAA[0][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBList\n",
    "import os\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "import pickle\n",
    "from Bio.PDB import PDBParser\n",
    "#from Bio.PDB.DSSP import DSSP\n",
    "from Bio import pairwise2\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"/Users/pamar/Desktop/projects/august/script/\")\n",
    "owd = os.getcwd()\n",
    "\n",
    "with open('../data/testSet.fasta', 'w') as out_file_big:\n",
    "    cont=0\n",
    "    with open('../data/Casp12Data.pickle','rb') as dataFile:\n",
    "        data = pickle.load(dataFile)\n",
    "        with open('../data/CASP.fasta', 'w') as out_file:\n",
    "            for i in range(data[0].shape[0]):\n",
    "                out_file.write(\">CASP_\"+str(cont)+\"\\n\")\n",
    "                out_file.write(reverseOneHot(data[0][i,:,0:20])+\"\\n\")\n",
    "                out_file_big.write(\">CASP_\"+str(cont)+\"\\n\")\n",
    "                out_file_big.write(reverseOneHot(data[0][i,:,0:20])+\"\\n\")\n",
    "                cont+=1\n",
    "            \n",
    "            \n",
    "    cont=0\n",
    "    with open('../data/CB513.pickle','r\n",
    "              b') as dataFile:\n",
    "        data = pickle.load(dataFile)\n",
    "\n",
    "        with open('../data/CB513.fasta', 'w') as out_file:\n",
    "            for i in range(data[0].shape[0]):\n",
    "                out_file.write(\">CB513_\"+str(cont)+\"\\n\")\n",
    "                out_file.write(reverseOneHot(data[0][i,:,0:20])+\"\\n\")\n",
    "                out_file_big.write(\">CB513_\"+str(cont)+\"\\n\")\n",
    "                out_file_big.write(reverseOneHot(data[0][i,:,0:20])+\"\\n\")\n",
    "                cont+=1\n",
    "    cont=0\n",
    "    with open('../data/TS115.pickle','rb') as dataFile:\n",
    "        data = pickle.load(dataFile)\n",
    "        with open('../data/TS115.fasta', 'w') as out_file:\n",
    "            for i in range(data[0].shape[0]):\n",
    "                out_file.write(\">TS115_\"+str(cont)+\"\\n\")\n",
    "                out_file.write(reverseOneHot(data[0][i,:,0:20])+\"\\n\")\n",
    "                out_file_big.write(\">TS115_\"+str(cont)+\"\\n\")\n",
    "                out_file_big.write(reverseOneHot(data[0][i,:,0:20])+\"\\n\")\n",
    "                cont+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b0ed7ac249be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mfinalSeqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseqs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblastFile\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".fasta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fasta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabelsRed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mfinalSeqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalSeqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblastFile\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_reduced.fasta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fasta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "\n",
    "def cleanDF(dataset,seq_threshold,length_threshold=0.5):\n",
    "    qseqid = list(dataset.iloc[:,0])\n",
    "    sseqid = list(dataset.iloc[:,1])\n",
    "    pident = list(dataset.iloc[:,2])\n",
    "    length = list(dataset.iloc[:,3])\n",
    "    qlen   = list(dataset.iloc[:,4])\n",
    "    redundantSeqs = []\n",
    "    for i in range(len(qseqid)):\n",
    "        if ((pident[i] > seq_threshold) and (length_threshold * qlen[i] < length[i])):\n",
    "            if qseqid[i] not in redundantSeqs:\n",
    "                #print(qseqid[i])\n",
    "                redundantSeqs.append(qseqid[i])\n",
    "    return redundantSeqs\n",
    "\n",
    "\n",
    "seqid = 25\n",
    "minCov=.50\n",
    "\n",
    "for blastFile in ['../data/dataset70','../data/dataset50','../data/dataset30']:\n",
    "\n",
    "    blastData = pd.read_csv(blastFile+'.blast.out',sep='\\t',header=None).iloc[:,0:5]\n",
    "\n",
    "    labelsRed = cleanDF(blastData,seqid,minCov)\n",
    "    finalSeqs=[]\n",
    "    for seqs in SeqIO.parse(blastFile+\".fasta\", \"fasta\"):\n",
    "        if seqs.id not in labelsRed:\n",
    "                finalSeqs.append(seqs)\n",
    "    SeqIO.write(finalSeqs, blastFile+\"_reduced.fasta\", \"fasta\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: starting\n",
      "PDB is 16PKA\n",
      "PDB is 12ASA\n",
      "Downloading PDB structure '12AS'...\n",
      "Downloading PDB structure '16PK'...\n",
      "main: unprocessed results <generator object _chain_from_iterable_of_lists at 0x7fa3bae94200>\n",
      "main: waiting for real results\n",
      "<Model id=0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 5676.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 5708.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 5740.\n",
      "  PDBConstructionWarning,\n",
      "/Users/pamar/anaconda2/envs/py36/lib/python3.6/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 5842.\n",
      "  PDBConstructionWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model id=0>\n",
      "main: results: [[array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])], [array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]]\n",
      "Finished with 30 redundancy data set\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OneHotEncode new data\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBList\n",
    "import os\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "import pickle\n",
    "import argparse\n",
    "from concurrent import futures\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='create NSP2 training sets.')\n",
    "\n",
    "parser.add_argument('--file', dest='file',\n",
    "                    help='fasta file to process')\n",
    "parser.add_argument('--procs', dest='procs', default=2, type=int,\n",
    "                    help='number of processors to use - default 2')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "def DSSPseq(dssp,chain=\"A\"):\n",
    "    \"\"\"This function returns the sequence of a given chain in a dssp object\"\"\"\n",
    "\n",
    "    return ''.join([dssp.property_dict[x][1] for x in dssp.keys() if x[0]==chain ])\n",
    "\n",
    "def fastaSeq(fasta, chain=\"A\"):\n",
    "\n",
    "    from Bio import SeqIO\n",
    "    import re\n",
    "    for record in SeqIO.parse(fasta, \"fasta\"):\n",
    "        outer = re.compile(\"[Cc]hains? ([A-Z,]+)\")\n",
    "        m = outer.search(record.description.split(\"|\")[1])\n",
    "        if re.search(chain,m.groups()[0]):\n",
    "            return record.seq\n",
    "    return ''\n",
    "\n",
    "def oneHot(residue):\n",
    "    mapping = dict(zip(\"ACDEFGHIKLMNPQRSTVWY\", range(20)))\n",
    "    if residue in \"ACDEFGHIKLMNPQRSTVWY\":\n",
    "        return np.eye(20)[mapping[residue]]\n",
    "    else:\n",
    "        return np.zeros(20)\n",
    "\n",
    "def dssp8(classLetter):    \n",
    "    mapping = dict(zip(\"GHIBESTC\", range(8))) \n",
    "    if classLetter in \"GHIBESTC\":\n",
    "        return np.eye(8)[mapping[classLetter]]\n",
    "    else:\n",
    "        return np.zeros(8)\n",
    "    \n",
    "    \n",
    "def fasta2dssp(pdb,seq2):\n",
    "    currentStructure = pdb[:4]\n",
    "    print(\"PDB is\",pdb)\n",
    "    pdbl.retrieve_pdb_file(currentStructure,pdir='../data/PDB',file_format='pdb')\n",
    "    try:\n",
    "        structure = p.get_structure(currentStructure,'../data/PDB/pdb' + currentStructure + '.ent')\n",
    "    except FileNotFoundError:\n",
    "        return [[],[]]\n",
    "    model = structure[0]\n",
    "    print(model)\n",
    "    dssp = DSSP(model, '../data/PDB/pdb' + currentStructure + '.ent')\n",
    "    seq1=DSSPseq(dssp,pdb[-1])\n",
    "    #seq2=seqDict[pdb]\n",
    "    alignments = pairwise2.align.globalms(seq1, seq2,1,-1,-5,-.5,penalize_extend_when_opening=True,penalize_end_gaps=False)\n",
    "    dsspCont=0\n",
    "    dsspList=[]\n",
    "    a = np.array([])\n",
    "    b = np.array([])\n",
    "    os.remove('../data/PDB/pdb'+currentStructure+'.ent')\n",
    "    dsspCont = 0\n",
    "    for i in range(len(alignments[0][0])):\n",
    "        if alignments[0][0][i] == '-':\n",
    "            a = np.hstack((a,oneHot(alignments[0][1][i])))\n",
    "            b = np.hstack((b,dssp8('-')))\n",
    "        else:\n",
    "            #this is a norml element, add dssp to it\n",
    "            a = np.hstack((a,oneHot(alignments[0][1][i])))\n",
    "            #OHCSSList.append([dssp.property_dict[dssp.keys()[dsspCont]][2]])\n",
    "            c = dssp.property_dict[dssp.keys()[dsspCont]][2]\n",
    "            b = np.hstack((b,dssp8(c)))\n",
    "            #print(c)\n",
    "            dsspCont+=1\n",
    "    return [a.reshape(len(alignments[0][0]),20),b.reshape(len(alignments[0][0]),8)]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "from Bio.PDB import PDBParser\n",
    "#from Bio.PDB.DSSP import DSSP\n",
    "from Bio import pairwise2\n",
    "#\n",
    "#urlBase='https://files.rcsb.org/download/'\n",
    "#fastaBase='https://www.rcsb.org/fasta/entry/'\n",
    "\n",
    "SeqID30 = []\n",
    "Sequence30 = []\n",
    "file = args.file\n",
    "procs=args.procs\n",
    "for seq1 in SeqIO.parse(file, \"fasta\"):\n",
    "    Sequence30.append(str(seq1.seq))\n",
    "    SeqID30.append(seq1.id)\n",
    "SeqDict30 = dict(zip(SeqID30,Sequence30))\n",
    "p = PDBParser()\n",
    "pdbl=PDBList()\n",
    "OHCAA = []\n",
    "OHCSS = []\n",
    "# for pdb in SeqID30:\n",
    "#     currentStructure = pdb[:4]\n",
    "#     val=fasta2dssp(currentStructure,SeqDict30)\n",
    "#     OHCAA.append(val[0])\n",
    "#     OHCSS.append(val[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "ex = futures.ProcessPoolExecutor(max_workers=procs)\n",
    "#ex = futures.ProcessPoolExecutor(max_workers=2)\n",
    "\n",
    "\n",
    "print('main: starting')\n",
    "results = ex.map(fasta2dssp, SeqID30,Sequence30)\n",
    "print('main: waiting for real results')\n",
    "real_results = list(results)\n",
    "\n",
    "[OHCAA,OHCSS]=list(zip(*real_results))\n",
    "OHCAA=[np.array(x) for x in OHCAA]\n",
    "OHCSS=[np.array(x) for x in OHCSS]\n",
    "\n",
    "maxlength = max([x.shape[0] for x in OHCAA])\n",
    "# Extrapolating sequences\n",
    "for i in range(len(OHCAA)):\n",
    "    if maxlength > OHCAA[i].shape[0]:\n",
    "        diff = np.zeros((maxlength-OHCAA[i].shape[0],20))\n",
    "        OHCAA[i] = np.vstack((OHCAA[i],diff))\n",
    "        diff = np.zeros((maxlength-OHCSS[i].shape[0],8))\n",
    "        OHCSS[i] = np.vstack((OHCSS[i],diff))\n",
    "pickle.dump([OHCAA,OHCSS],open(file+'.pickle','wb'))\n",
    "#print(OHCAA)\n",
    "#print(OHCSS)\n",
    "print('Finished with {} redundancy data set'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "[OHCAA,OHCSS]=list(zip(*real_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OHCSS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVKALPDAQFEVVHSLAKWKRQTLGQHDFSAGEGLYTHMKALRPDEDRLSPLHSVYVDQWDWERVMGDGERQFSTLKSTVEAIWAGIKATEAAVSEEFGLAPFLPDQIHFVHSQELLSRYPDLDAKGRERAIAKDLGAVFLVGIGGKLSDGHRHDVRAPDYDDWSTPSELGHAGLNGDILVWNPVLEDAFELSSMGIRVDADTLKHQLALTGDEDRLELEWHQALLRGEMPQTIGGGIGQSRLTMLLLQLPHIGQVQAGVWPAAVRESVPSLL',\n",
       " 'EKKSINECDLKGKKVLIRVDFNVPVKNGKITNDYRIRSALPTLKKVLTEGGSCVLMSHLGRPKGIPMAQAGKIRSTGGVPGFQQKATLKPVAKRLSELLLRPVTFAPDCLNAADVVSKMSPGDVVLLENVRFYKEEGSKKAKDREAMAKILASYGDVYISDAFGTAHRDSATMTGIPKILGNGAAGYLMEKEISYFAKVLGNPPRPLVAIVGGAKVSDKIQLLDNMLQRIDYLLIGGAMAYTFLKAQGYSIGKSKCEESKLEFARSLLKKAEDRKVQVILPIDHVCHTEFKAVDSPLITEDQNIPEGHMALDIGPKTIEKYVQTIGKCKSAIWNGPMGVFEMVPYSKGTFAIAKAMGRGTHEHGLMSIIGGGDSASAAELSGEAKRMSHVSTGGGASLELLEGKTLPGVTVLDDK']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sequence30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import sklearn.model_selection as model_selection\n",
    "import pickle\n",
    "# Defining a function to obtain the prediction accuracies\n",
    "def getAccuracy(predictions,targets):\n",
    "targets = np.squeeze(targets.permute(0,2,1).argmax(dim=2).view(-1,targets.shape[0]*ta pred = np.squeeze(softmax(predictions.permute(0,2,1),dim=2).argmax(dim=2).view(-1,pre correct = 0\n",
    "pred = pred[targets != 8]\n",
    "targets = targets[targets != 8] correct = (pred == targets).sum() return correct/np.size(targets)\n",
    "### Load train and test data\n",
    "traindata = pickle.load(open('train.pickle','rb'))\n",
    "X = traindata[0][:,:,0:20]\n",
    "y = traindata[0][:,:,57:65]\n",
    "CASP12data = pickle.load(open('Casp12Data.pickle','rb')) X_casp = CASP12data[0][:,:,0:20]\n",
    "y_casp = CASP12data[0][:,:,57:65]\n",
    "TS115data = pickle.load(open('TS115.pickle','rb')) X_TS115 = TS115data[0][:,:,0:20]\n",
    "y_TS115 = TS115data[0][:,:,57:65]\n",
    "CB513data = pickle.load(open('CB513.pickle','rb')) X_CB513 = CB513data[0][:,:,0:20]\n",
    "y_CB513 = CB513data[0][:,:,57:65]\n",
    "https://colab.research.google.com/drive/1wbAxkPjUyiX68kEKdazAHBFgn751f7D7#scrollTo=9dXoZb1A7g0h&printMode=true 4/11\n",
    "r d\n",
    " 14.12.2020 Data redundancy in developing deep learning tools on biological data.ipynb - Colaboratory\n",
    "# Since smaller sequences are extrapolated with zeros to match the length of # the longest sequence in the dataset we need to add a \"ninth\" class of SS in # order not to make Pythn confuse the first class with the extrapolated ones if y.shape[2] == 8:\n",
    "EPzeros = np.expand_dims(np.zeros((y.shape[1])),1)\n",
    "EPzeros_casp = np.expand_dims(np.zeros((y_casp.shape[1])),1)\n",
    "EPzeros_TS115 = np.expand_dims(np.zeros((y_TS115.shape[1])),1)\n",
    "EPzeros_CB513 = np.expand_dims(np.zeros((y_CB513.shape[1])),1)\n",
    "y = np.asarray([np.hstack((y[i],EPzeros)) for i in range(y.shape[0])])\n",
    "y_casp = np.asarray([np.hstack((y_casp[i],EPzeros_casp)) for i in range(y_casp.shape[ y_TS115 = np.asarray([np.hstack((y_TS115[i],EPzeros_TS115)) for i in range(y_TS115.sh y_CB513 = np.asarray([np.hstack((y_CB513[i],EPzeros_CB513)) for i in range(y_CB513.sh for i in range(y.shape[0]):\n",
    "for j in range(y[i].shape[0]): if np.all(y[i][j,:] == 0):\n",
    "y[i][j,:][-1] = 1 for i in range(y_casp.shape[0]):\n",
    "for j in range(y_casp[i].shape[0]): if np.all(y_casp[i][j,:] == 0):\n",
    "y_casp[i][j,:][-1] = 1 for i in range(y_TS115.shape[0]):\n",
    "for j in range(y_TS115[i].shape[0]): if np.all(y_TS115[i][j,:] == 0):\n",
    "y_TS115[i][j,:][-1] = 1 for i in range(y_CB513.shape[0]):\n",
    "for j in range(y_CB513[i].shape[0]): if np.all(y_CB513[i][j,:] == 0):\n",
    "y_CB513[i][j,:][-1] = 1\n",
    "# Converting to tensors for PyTorch\n",
    "X_train = torch.tensor(X, dtype = torch.float)\n",
    "y_train = torch.tensor(y, dtype = torch.float).permute(0,2,1) X_casp = torch.tensor(X_casp,dtype = torch.float)\n",
    "y_casp = torch.tensor(y_casp,dtype=torch.float).permute(0,2,1) X_TS115 = torch.tensor(X_TS115,dtype=torch.float)\n",
    "y_TS115 = torch.tensor(y_TS115,dtype=torch.float).permute(0,2,1) X_CB513 = torch.tensor(X_CB513,dtype=torch.float)\n",
    "y_CB513 = torch.tensor(y_CB513,dtype=torch.float).permute(0,2,1)\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "self.data = X self.targets = y\n",
    "    def __getitem__(self, index):\n",
    "x = self.data[index]\n",
    "y = self.targets[index]\n",
    "return x, y\n",
    "def __len__(self): return len(self.data)\n",
    "# Creating dataloaders for training in batches for later.\n",
    "batch_size = 65\n",
    "TrainLoader = DataLoader(MyDataset(X_train,y_train),batch_size=batch_size) CASPLoader = DataLoader(MyDataset(X_casp,y_casp),batch_size=3)\n",
    " https://colab.research.google.com/drive/1wbAxkPjUyiX68kEKdazAHBFgn751f7D7#scrollTo=9dXoZb1A7g0h&printMode=true 5/11\n",
    "0 a a\n",
    "\n",
    " 14.12.2020 Data redundancy in developing deep learning tools on biological data.ipynb - Colaboratory\n",
    "TS115Loader = DataLoader(MyDataset(X_TS115,y_TS115),batch_size=batch_size) CB513Loader = DataLoader(MyDataset(X_CB513,y_CB513),batch_size=batch_size)\n",
    "# Defining the CNN architecture\n",
    " channels = 20 kernel_size_conv1 padding_conv1 = 7 stride_conv1 = 1 kernel_size_conv2 padding_conv2 = 4 stride_conv2 = 1 kernel_size_conv3 padding_conv3 = 2 stride_conv3 = 1\n",
    "= 15 = 9 = 5\n",
    "class Net(nn.Module): def __init__(self):\n",
    "''''''\n",
    "super().__init__()\n",
    "self.conv1 = nn.Sequential( nn.Conv1d(in_channels=channels,out_channels=25,kernel_size=kernel_size_conv1 nn.ReLU(),\n",
    "nn.BatchNorm1d(25),\n",
    "nn.Dropout(p=0.5))\n",
    "self.conv2 = nn.Sequential(\n",
    "nn.Conv1d(in_channels=25, out_channels=35, kernel_size=kernel_size_conv2, str nn.ReLU(),\n",
    "nn.BatchNorm1d(35),\n",
    "nn.Dropout(p=0.5))\n",
    "self.conv3 = nn.Sequential(\n",
    "nn.Conv1d(in_channels=35,out_channels=40, kernel_size=kernel_size_conv3, stri nn.ReLU(),\n",
    "nn.BatchNorm1d(40),\n",
    "nn.Dropout(p=0.5))\n",
    "# Using a convolutional layer instead of linear. self.fc1_encode1 = nn.Sequential(\n",
    "nn.Conv1d(in_channels=40,out_channels=9,kernel_size=1,stride=1,padding=0,bias\n",
    "nn.BatchNorm1d(9) )\n",
    "def forward(self, x):\n",
    "x = self.conv1(x)\n",
    "x = self.conv2(x)\n",
    "x = self.conv3(x)\n",
    "x = self.fc1_encode1(x) return x\n",
    "net = Net()\n",
    "# Defining criterion and optimizer\n",
    "https://colab.research.google.com/drive/1wbAxkPjUyiX68kEKdazAHBFgn751f7D7#scrollTo=9dXoZb1A7g0h&printMode=true 6/11\n",
    ",\n",
    "i\n",
    "d\n",
    "=\n",
    "\n",
    " 14.12.2020 Data redundancy in developing deep learning tools on biological data.ipynb - Colaboratory\n",
    "import torch.optim as optim\n",
    " def criterion(input, target):\n",
    "labels = torch.argmax(target,2) #print(input)\n",
    "#print(\"T\")\n",
    "#print(input.shape)\n",
    "#print(labels.shape)\n",
    "return nn.CrossEntropyLoss()(input, labels)\n",
    "# Using thr ADAM optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001,betas=(0.85,0.95),weight_decay=1e-6)\n",
    "# Training for 50 epochs\n",
    "num_epoch = 50 # Your code here!\n",
    "# Creating lists to store loss and accuracies in train_loss = []\n",
    "train_accuracy = []\n",
    "casp_loss = []\n",
    "casp_accuracy = []\n",
    "TS115_loss = []\n",
    "TS115_accurac = []\n",
    "CB513_loss = []\n",
    "CB513_accuracy = []\n",
    "# Training\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "print('Epoch ',epoch+1,' of ',num_epoch) running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "net.train()\n",
    "print('Training net') for data in TrainLoader:\n",
    "inputs, labels = data\n",
    "# Wrap them in Variable\n",
    "inputs, labels = Variable(inputs.permute(0,2,1)), Variable(labels) # zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "# forward + backward + optimize\n",
    "outputs = net(inputs)\n",
    "running_acc += getAccuracy(outputs,labels)\n",
    "loss = criterion(outputs,labels.permute(0,2,1))\n",
    "loss.backward()\n",
    "running_loss += loss.data.numpy()\n",
    "optimizer.step()\n",
    "# Nromalizing the error and append it to list. train_loss.append(running_loss/len(TrainLoader))\n",
    "# Turning off gradients while validating on test set. net.eval()\n",
    "with torch.no_grad():\n",
    "running_loss = 0.0 running_acc = 0.0 print('Testing on CASP') for data in CASPLoader:\n",
    "inputs, labels = data\n",
    "tt t( ibl(i t t(02)))\n",
    "https://colab.research.google.com/drive/1wbAxkPjUyiX68kEKdazAHBFgn751f7D7#scrollTo=9dXoZb1A7g0h&printMode=true 7/11\n",
    "\n",
    "14.12.2020 Data redundancy in developing deep learning tools on biological data.ipynb - Colaboratory\n",
    "outputs = net(Variable(inputs.permute(0,2,1))) loss = criterion(outputs,labels.permute(0,2,1)) running_acc += getAccuracy(outputs,labels) running_loss += loss.data.numpy()\n",
    "casp_loss.append(running_loss/len(CASPLoader)) casp_accuracy.append(running_acc/len(CASPLoader)) running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "print('Testing on TS115') for data in TS115Loader:\n",
    "inputs, labels = data\n",
    "outputs = net(Variable(inputs.permute(0,2,1))) loss = criterion(outputs,labels.permute(0,2,1)) running_acc += getAccuracy(outputs,labels) running_loss += loss.data.numpy()\n",
    "TS115_loss.append(running_loss/len(TS115Loader)) TS115_accuracy.append(running_acc/len(TS115Loader)) running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "print('Testing on CB513') for data in CB513Loader:\n",
    "inputs, labels = data\n",
    "outputs = net(Variable(inputs.permute(0,2,1))) loss = criterion(outputs,labels.permute(0,2,1)) running_acc += getAccuracy(outputs,labels) running_loss += loss.data.numpy()\n",
    "CB513_loss.append(running_loss/len(CB513Loader))\n",
    "CB513_accuracy.append(running_acc/len(CB513Loader)) print('Finished Training')\n",
    "scale = list(range(1,num_epoch+1))\n",
    "# Plotting of results\n",
    "fig, axs = plt.subplots(1, 2, constrained_layout=False) axs[0].plot(scale, train_loss,label='Train') axs[0].plot(scale, casp_loss,label='CASP12') axs[0].plot(scale, TS115_loss,label='TS115') axs[0].plot(scale, CB513_loss,label='CB513') axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend(loc='upper right')\n",
    "axs[1].plot(scale, train_accuracy,label='Train') axs[1].plot(scale, casp_accuracy,label='CASP12') axs[1].plot(scale, TS115_accuracy,label='TS115') axs[1].plot(scale, CB513_accuracy,label='CB513') axs[1].set_xlabel('Epochs') axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend(loc='lower right') fig.suptitle('Dropout, BN and L2', fontsize=16,y=1.05) plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
